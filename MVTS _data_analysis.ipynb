{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from datetime import datetime\n",
    "\n",
    "class MVTSSample:\n",
    "    \n",
    "    def __init__(self, flare_type:str, start_time:datetime, end_time:datetime, data:DataFrame):\n",
    "        self._flare_type = flare_type\n",
    "        self._start_time = start_time\n",
    "        self._end_time = end_time\n",
    "        self._data = data\n",
    "    \n",
    "    def get_flare_type(self):\n",
    "        return self._flare_type\n",
    "    \n",
    "    def get_start_time(self):\n",
    "        return self._start_time\n",
    "    \n",
    "    def get_end_time(self):\n",
    "        return self._end_time\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self._data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "def read_non_flare_mvts(data_dir:str, file_name:str) -> MVTSSample:\n",
    "    match  = re.findall(r'(.*)_s(.*)_e(.*)',file_name[:-4])\n",
    "    s_t = match[0][1]\n",
    "    #print(s_t)\n",
    "    start_time = datetime.strptime(s_t.replace('T',' ').replace('_',':'), '%Y-%m-%d %H:%M:%S')\n",
    "    e_t = match[0][2]\n",
    "    #print(e_t)\n",
    "    #print(match)\n",
    "    end_time =  datetime.strptime(e_t.replace('T',' ').replace('_',':'), '%Y-%m-%d %H:%M:%S')\n",
    "    if(match[0][0][0] == 'F'):\n",
    "        flare_type = match[0][0][0:2]\n",
    "    else:\n",
    "        flare_type = match[0][0][0]\n",
    "    \n",
    "    path = data_dir+\"\\\\\"+ file_name\n",
    "    data = pd.read_csv(path,delimiter='\\t')\n",
    "    data_frame = pd.DataFrame(data)\n",
    "    \n",
    "    return MVTSSample(flare_type,start_time,end_time,data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_flare_mvts(data_dir:str, file_name:str) -> MVTSSample:\n",
    "    match  = re.findall(r'(.*)_s(.*)_e(.*)',file_name[:-4])\n",
    "    s_t = match[0][1]\n",
    "    start_time = datetime.strptime(s_t.replace('T',' ').replace('_',':'), '%Y-%m-%d %H:%M:%S')\n",
    "    e_t = match[0][2]\n",
    "    end_time =  datetime.strptime(e_t.replace('T',' ').replace('_',':'), '%Y-%m-%d %H:%M:%S')\n",
    "    if(match[0][0][0] == 'F'):\n",
    "        flare_type = match[0][0][0:2]\n",
    "    else:\n",
    "        flare_type = match[0][0][0]\n",
    "    \n",
    "    path = data_dir+\"\\\\\"+ file_name\n",
    "    data = pd.read_csv(path,delimiter='\\t')\n",
    "    data_frame = pd.DataFrame(data)\n",
    "    \n",
    "    return MVTSSample(flare_type,start_time,end_time,data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'C:\\Users\\Krishna Rukmini\\Downloads\\partition1_instances\\partition1_instances (1).tar\\partition1\\NF'\n",
    "file_name = \"B1.0@13_Primary_ar10_s2010-05-02T23_12_00_e2010-05-03T11_00_00.csv\"\n",
    "results = read_non_flare_mvts(data_dir, file_name)\n",
    "results.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in df.columns:\n",
    "    if df[val].nunique() > 10 :\n",
    "        print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_descriptive_features(data:DataFrame)-> DataFrame:\n",
    "    variates_to_calc_on = [ 'R_VALUE','TOTUSJH','TOTBSQ','TOTPOT','TOTUSJZ','ABSNJZH','SAVNCPP',\n",
    "                           'USFLUX','TOTFZ','MEANPOT','EPSZ','MEANSHR','SHRGT45','MEANGAM','MEANGBT',\n",
    "                           'MEANGBZ','MEANGBH','MEANJZH','TOTFY','MEANJZD','MEANALP','TOTFX']\n",
    "\n",
    "    result = []\n",
    "    total = []\n",
    "    features_to_return =[]\n",
    "    features=[]\n",
    "\n",
    "    for values in variates_to_calc_on:\n",
    "        result.append(np.float64(np.median(data[values])))\n",
    "        result.append(np.float64(np.mean(data[values])))\n",
    "        result.append(np.float64(np.std(data[values])))\n",
    "        result.append(np.float64(np.min(data[values])))\n",
    "        result.append(np.float64(np.max(data[values])))\n",
    "        result.append(np.float64(np.var(data[values])))\n",
    "        \n",
    "        features.append(values+\"_MEDIAN\")\n",
    "        features.append(values+\"_MEAN\")\n",
    "        features.append(values+\"_STDDEV\")\n",
    "        features.append(values+\"_MIN\")\n",
    "        features.append(values+\"_MAX\")\n",
    "        features.append(values+\"_vARIANCE\")\n",
    "        \n",
    "    total.append(result)\n",
    "    features_to_return.append(features)\n",
    "\n",
    "    #print(features_to_return)\n",
    "    \n",
    "    Result_df = pd.DataFrame(total,columns=features_to_return[0])\n",
    "    \n",
    "    return Result_df,features_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,features = calculate_descriptive_features(results.get_data())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_return = features[0]\n",
    "features_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(partition_location:str, abt_name:str):\n",
    "    Descriptive_frame= pd.DataFrame([],columns = features_to_return )\n",
    "    flag = 0\n",
    "    count_FL = 0\n",
    "    count_NF = 0\n",
    "    dir_Name = partition_location;\n",
    "    listOfFiles = List_Of_Files(dir_Name)\n",
    "    print(listOfFiles)\n",
    "    for i in listOfFiles[1:]:\n",
    "        \n",
    "        for ele in os.listdir(i):\n",
    "            if(flag == 0):\n",
    "                data_result = read_flare_mvts(dir_Name+'\\FL',ele)\n",
    "                #print(count_FL)\n",
    "                #count_FL = count_FL+1\n",
    "            else:\n",
    "                data_result = read_non_flare_mvts(dir_Name+'\\\\NF',ele)\n",
    "                #print(count_NF)\n",
    "                #count_NF = count_NF +1\n",
    "            Descriptive_features, f = calculate_descriptive_features(data_result.get_data())\n",
    "            Descriptive_features.insert(0,\"FLARE_TYPE\",data_result.get_flare_type())\n",
    "            Descriptive_frame = Descriptive_frame.append(Descriptive_features)\n",
    "        flag = flag+1\n",
    "        Descriptive_frame.to_csv(abt_name,index = False,header = True)\n",
    "    return Descriptive_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def List_Of_Files(dir_Name): \n",
    "    listOfFile = [x[0]  for x in os.walk(dir_Name)]\n",
    "    return listOfFile     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = process_partition(r\"C:\\Users\\Krishna Rukmini\\Downloads\\partition1_instances\\partition1_instances (1).tar\\partition1\",r\"C:\\Users\\Krishna Rukmini\\Downloads\\partition1\\all_names.csv\")\n",
    "print(result)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\Krishna Rukmini\\Downloads\\partition1\\all_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_summary_for(feature_name:str, data):\n",
    "    summary_feature_names = ['Feature Name', 'Cardinality', 'Non-null Count', 'Null Count', 'Min', '25th', 'Mean', \n",
    "                             '50th', '75th', 'Max', 'std. Dev','Outlier Count Low', 'Outlier Count High']\n",
    "    \n",
    "    result = []\n",
    "    data.sort_values(by = feature_name, inplace = True)\n",
    "    \n",
    "    result.append(feature_name) #'Feature Name'\n",
    "    result.append(data[feature_name].nunique()) # 'Cardinality'\n",
    "    result.append(data[feature_name].count()) #'Non-null Count'\n",
    "    result.append(data[feature_name].isnull().sum()) #'Null Count'\n",
    "    result.append(data[feature_name].min()) #'Min'\n",
    "    \n",
    "    Q1 = np.percentile(data[feature_name],25)\n",
    "    result.append(Q1) #'25th'\n",
    "    \n",
    "    Q2 = np.percentile(data[feature_name],50)\n",
    "    result.append(Q2) #'50th'\n",
    "    \n",
    "    Q3 = np.percentile(data[feature_name],75)\n",
    "    result.append(Q3) #'75th'\n",
    "    \n",
    "    result.append(data[feature_name].max()) #'Max'\n",
    "    result.append(data[feature_name].mean()) #'Mean'\n",
    "    result.append(data[feature_name].std()) #'std. Dev'\n",
    "    \n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    for val in data[feature_name]:\n",
    "        #print(val)\n",
    "        Outlier_Count_Low = 0\n",
    "        Outlier_Count_High = 0\n",
    "        if (val < Q1-(1.5*IQR)):\n",
    "            Outlier_Count_Low = Outlier_Count_Low + 1\n",
    "        elif (val > Q3+(1.5*IQR)):\n",
    "            Outlier_Count_High =Outlier_Count_High + 1\n",
    "    \n",
    "    result.append(Outlier_Count_Low)\n",
    "    result.append(Outlier_Count_High)\n",
    "    \n",
    "    \n",
    "    Result_df = pd.DataFrame([result],columns=summary_feature_names)\n",
    "    \n",
    "    return Result_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_quality_report(data):\n",
    "    excluded_columns = ['FLARE_TYPE']\n",
    "    \n",
    "    summary_feature_names = ['Feature Name', 'Cardinality', 'Non-null Count', 'Null Count', 'Min', '25th', 'Mean', \n",
    "                             '50th', '75th', 'Max','std. Dev', 'Outlier Count Low', 'Outlier Count High']\n",
    "    \n",
    "    Result_summary = pd.DataFrame(columns=summary_feature_names)\n",
    "    summary_table_df = pd.DataFrame(data.drop(columns= excluded_columns ,inplace= False))\n",
    "    \n",
    "    list_col = list(summary_table_df.columns) \n",
    "    for val in list_col:\n",
    "        Result_summary = pd.concat([Result_summary,calc_summary_for(val, summary_table_df)],ignore_index=True)\n",
    "    \n",
    "    #Result_summary = Result_summary.reindex(index=[i for i in range(Result_summary.shape[0])])\n",
    "    return Result_summary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "summary_table_full_partition = construct_quality_report(data)\n",
    "def drop_low_card_data(summary_table, data) -> None:\n",
    "    index_del = summary_table.loc[summary_table['Cardinality']<10].index\n",
    "    new_list = []\n",
    "    for ind in index_del:\n",
    "        new_list.append(summary_table[\"Feature Name\"][ind])\n",
    "    summary_table.drop(index_del,inplace= True)        \n",
    "    data.drop(new_list,axis = 1,inplace = True)\n",
    "\n",
    "drop_low_card_data(summary_table_full_partition, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_excessive_nan_data(summary_table, data) -> None:\n",
    "    col_null_del=[]\n",
    "    for val in data.columns:\n",
    "        if (data[val].isnull().sum() > len(data)/100):\n",
    "            col_null_del.append(val)\n",
    "    \n",
    "    data.drop(col_null_del,axis = 1,inplace = True)\n",
    "    \n",
    "    index_null_drp = []\n",
    "    for val in col_null_del:\n",
    "        index_null_drp.append(summary_table.loc[summary_table[\"Feature Name\"]==val].index)\n",
    "    \n",
    "    for i in index_null_drp:\n",
    "        summary_table.drop(i[0],inplace = True,axis = 0)\n",
    "\n",
    "drop_excessive_nan_data(summary_table_full_partition, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test data\n",
    "\n",
    "out_dir = \"C:/Users/Krishna Rukmini/Downloads\"\n",
    "out_summary_table_name = 'data_summary_table.csv'\n",
    "out_data = 'cleaned_partition1ExtractedFeatures.csv'\n",
    "\n",
    "summary_table_full_partition.to_csv(out_summary_table_name,header=True,index=False)\n",
    "data.to_csv(out_data,header=True,index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
